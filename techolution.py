# -*- coding: utf-8 -*-
"""Copy of techolution

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kaUL_T47BzOLA67Pu7SKhXeozbJVjm-V
"""

# !pip install PyPDF2
# !pip install phonenumbers
# !pip install transformers
# !pip install pytesseract


# from google.colab import drive
# drive.mount('/content/drive')

#@title Imports


import nltk
import datetime
import spacy
import re
import phonenumbers
import os
import PyPDF2
from urllib.parse import urlparse
import json
import pytesseract
from PIL import Image

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


nltk.download('punkt')
nltk.download('maxent_ne_chunker')
nltk.download('words')

from nltk.corpus import stopwords
nltk.download('stopwords')
nlp = spacy.load('en_core_web_sm')
STOPWORDS = set(stopwords.words('english'))

from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)

import torch
from transformers import AutoTokenizer, AutoModel,AutoModelForSeq2SeqLM

similaritytokenizer = AutoTokenizer.from_pretrained("sentence-transformers/multi-qa-mpnet-base-dot-v1")
similaritymodel = AutoModel.from_pretrained("sentence-transformers/multi-qa-mpnet-base-dot-v1")
summarytokenizer = AutoTokenizer.from_pretrained("Samir001/ResumeSummary-t5-Wang-Arora")
summarymodel = AutoModelForSeq2SeqLM.from_pretrained("Samir001/ResumeSummary-t5-Wang-Arora")

PHONE_REG = re.compile(r'[\+\(]?[1-9][0-9 .\-\(\)]{8,}[0-9]')
EMAIL_REG = re.compile(r'[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+')
URL_REG = r'(https?://\S+|www\.\S+|(\S+\.(?:com|org|net|edu|gov|info|io|co|ai|ly|me))/\S+)'



EDUCATION = [
            'BE','B.E.', 'B.E', 'BS', 'B.S',
            'ME', 'M.E', 'M.E.', 'MS', 'M.S',
            'BTECH', 'B.TECH', 'M.TECH', 'MTECH',
            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII', 'be', 'b.e.', 'b.e', 'bs', 'b.s',
    'me', 'm.e', 'm.e.', 'ms', 'm.s',
    'btech', 'b.tech', 'm.tech', 'mtech',
    'ssc', 'hsc', 'cbse', 'icse', 'x', 'xii', 'Bachelor', "Bachelor's", "Bachelors", "masters", "Master's"
        ]

#@title Util / Sub Functions



def clean_and_preprocess_text(text):
    text = text.replace("\n", " ").strip()
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)
    text = text.lower()
    text = ' '.join(text.split())
    return text


def extract_phone_number(resume_text):
    phone = re.findall(PHONE_REG, resume_text)
    if phone:
        number = ''.join(phone[0])
        return number
    else:
        return None




def extract_email(resume_text):
    return re.findall(EMAIL_REG, resume_text)

def extract_skills(text):
    text = clean_and_preprocess_text(text)
    doc = nlp(text)

    word_frequencies = {}

    for token in doc:
        if not token.is_stop and not token.is_punct:
            lemma = token.lemma_
            if lemma in word_frequencies:
                word_frequencies[lemma] += 1
            else:
                word_frequencies[lemma] = 1

    sorted_words = sorted(word_frequencies, key=word_frequencies.get, reverse=True)
    sorted_words2 = [word for word in sorted_words if not word.isnumeric()]
    keywords = sorted_words2[:15]
    return keywords

# def extract_skills(input_text):

#     SKILLS_DB = [
#     "Python", "Java", "C++", "JavaScript", "SQL", "HTML", "CSS", "Ruby", "PHP", "Swift",
#     "React", "Angular", "Vue.js", "Node.js", "Django", "Flask", "Spring", "Ruby on Rails",
#     "PHP Laravel", "SwiftUI", "Machine Learning", "Deep Learning", "Data Analysis", "Data Science",
#     "Artificial Intelligence", "Natural Language Processing", "Computer Vision", "Big Data", "Hadoop",
#     "Spark", "MongoDB", "MySQL", "PostgreSQL", "AWS", "Azure", "Google Cloud", "DevOps", "Docker",
#     "Kubernetes", "Git", "Jenkins", "CI/CD", "Front-end Development", "Back-end Development",
#     "Full Stack Development", "Mobile App Development", "Web Development", "UI/UX Design",
#     "Project Management", "Agile", "Scrum", "Kanban", "Leadership", "Problem Solving",
#     "Teamwork", "Communication", "Time Management", "Customer Service", "Sales", "Marketing",
#     "Digital Marketing", "Content Marketing", "SEO", "Social Media", "Public Speaking",
#     "Presentation Skills", "Negotiation", "Financial Analysis", "Accounting", "Business Strategy",
#     "Entrepreneurship", "E-commerce", "Product Management", "Quality Assurance", "Cybersecurity",
#     "Network Security", "Penetration Testing", "Ethical Hacking", "Blockchain", "Cryptocurrency",
#     "Game Development", "Virtual Reality (VR)", "Augmented Reality (AR)", "IoT", "Robotics"
#     ]

#     stop_words = set(nltk.corpus.stopwords.words('english'))
#     word_tokens = nltk.tokenize.word_tokenize(input_text)

#     # remove the stop words
#     filtered_tokens = [w for w in word_tokens if w not in stop_words]

#     # remove the punctuation
#     filtered_tokens = [w for w in word_tokens if w.isalpha()]

#     # generate bigrams and trigrams (such as artificial intelligence)
#     bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))

#     # we create a set to keep the results in.
#     found_skills = []

#     # we search for each token in our skills database
#     for token in filtered_tokens:
#         if token.lower() in SKILLS_DB:
#             found_skills.append(token)

#     # we search for each bigram and trigram in our skills database
#     for ngram in bigrams_trigrams:
#         if ngram.lower() in SKILLS_DB:
#             found_skills.append(ngram)
#     return found_skills


def extract_links(resume_text):
    links = re.findall(URL_REG, resume_text)

    linkedin_links = []
    social_links = []
    github_links = []
    other_links = []
    try:
      for url in links[0]:
          if 'linkedin.com' in url:
              linkedin_links.append(url)
          elif 'twitter.com' in url or 'facebook.com' in url:
              social_links.append(url)
          elif 'github.com' in url:
              github_links.append(url)
          else:
              other_links.append(url)

      print("LinkedIn Links:", linkedin_links)
      print("Social Media Links:", social_links)
      print("GitHub Links:", github_links)
      print("Other Links:", other_links)

    except:
      return None


# def extract_education(text):
#     education = []

#     pattern = r"(?i)(?:Bsc|\bB\.\w+|\bM\.\w+|\bPh\.D\.\w+|\bBachelor(?:'s)?|\bMaster(?:'s)?|\bPh\.D)\s(?:\w+\s)*\w+"
#     matches = re.findall(pattern, text)
#     for match in matches:
#         education.append(match.strip())

#     return education

#@title Educaiton

def extract_education(resume_text):
    nlp_text = nlp(resume_text)
    nlp_text = [sent.text.strip() for sent in nlp_text.sents]
    edu = {}
    try:
      for index, text in enumerate(nlp_text):
          for tex in text.split():
              tex = re.sub(r'[?|$|.|!|,]', r'', tex)
              if tex in EDUCATION and tex not in STOPWORDS:
                  edu[tex] = text + nlp_text[index + 1]
      education = []
      for key in edu.keys():
          year = re.search(re.compile(r'(((20|19)(\d{2})))'), edu[key])
          if year:
              education.append((key, ''.join(year[0])))
          else:
              education.append(key)
      return education
    except:
      return None

#@title Similarity

def find_match6(similaritytokenizer, similaritymodel, text, job_description):
    scoresum = []
    paras = []
    paras.append(text)

    def cls_pooling(model_output):
        return model_output.last_hidden_state[:,0]

    def encode(texts):
        encoded_input = similaritytokenizer(texts, padding=True, truncation=True, return_tensors='pt')
        with torch.no_grad():
            model_output = similaritymodel(**encoded_input, return_dict=True)
        embeddings = cls_pooling(model_output)
        return embeddings

    query_emb = encode(job_description)
    doc_emb = encode(paras)
    response = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()

    print("\n\n\nSCORE Resume vs JD: ", response[0])
    scoresum.append(response[0])

    keywords = extract_skills(text)
    keywordsstr = ' '.join([str(elem) for elem in keywords])

    query_emb = encode(job_description)
    doc_emb = encode(keywordsstr)
    response2 = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()
    scoresum.append(response2[0])

    print("\n\n\nSCORE SkillSet vs JD: ", response2[0])
    return scoresum

#@title Name

def extract_name(resume_text):
    nlp_text = nlp(resume_text)

    # Define patterns for capturing full names with optional middle names
    patterns = [
        [{'POS': 'PROPN', 'OP': '+'}],  # One or more proper nouns (First Name, Middle Name, Last Name)
        [{'POS': 'PROPN', 'OP': '+'}, {'POS': 'PROPN', 'OP': '+'}]  # First Name, optional Middle Name, Last Name
    ]

    for pattern in patterns:
        matcher.add('NAME', [pattern], on_match=None)

    matches = matcher(nlp_text)
    names = []

    for match_id, start, end in matches:
        span = nlp_text[start:end]
        names.append(span.text)

    return names[:3]

def operate(text, job_description):
    finaldict = {}

    print("Name:")
    finaldict["Name(s)"] = extract_name(text)
    # print(finaldict["Name"])

    print("\nEmail:")
    finaldict["Email(s)"] = extract_email(text)

    print("\nPhone Number:")
    finaldict["Phone_Number(s)"] = extract_phone_number(text)

    print("\nLinks (LinkedIn, Social, Other, GitHub):\n")
    links = extract_links(text)
    finaldict["Links"] = links
    print(links)

    stop = stopwords.words('english')
    text = " ".join([word for word in text.split() if word not in (stop)])

    print("\nSkills:")
    skills = extract_skills(text)
    skills_str = ", ".join(skills)
    finaldict["Skills"] = skills_str
    print(skills_str)

    print("\nExperience:")
    finaldict["Experience"] = " "

    print("\n\n\nRating Against Job Description:")
    similarity_score = find_match6(similaritytokenizer, similaritymodel, text, job_description)
    finaldict["Scores"] = similarity_score


    print("\nSummary:")
    summary = summarize(summarytokenizer, summarymodel, text)
    finaldict["Summary"] = summary
    print(summary)

    print("\nEducation:")
    education = extract_education(text)
    finaldict["Education"] = education

    return finaldict
        
def extract_text_from_image(path):
    image = Image.open(path)
    text = pytesseract.image_to_string(image)
    return text


def iterate(path1, job_description):
    dic2 ={}
    for filename in os.listdir(path1):
      if filename.endswith(".pdf"):
          print("""-----------------------------------------
          """ + filename)
          pdf_file_path = os.path.join(path1, filename)

          with open(pdf_file_path, "rb") as pdf_file:
              pdf_reader = PyPDF2.PdfReader(pdf_file)
              text = ""

              for page_num in range(len(pdf_reader.pages)):
                  page = pdf_reader.pages[page_num]
                  text += page.extract_text()

              dic2[str(filename)] = operate(text, job_description)
            #   return dic


    for filename in os.listdir(path1):
      if filename.endswith((".png", ".jpg", ".jpeg", ".bmp")):
          print("-----------------------------------------\n" + filename)
          image_path = os.path.join(path1, filename)
          text = extract_text_from_image(image_path)
          dic2["filenae"] = operate(text, job_description)

    return dic2
    

def summarize(summarytokenizer, summarymodel, resume):
  text = clean_and_preprocess_text(resume)
  inputs = summarytokenizer(text, return_tensors="pt")
  input_ids = inputs.input_ids

  outputs = summarymodel.generate(input_ids)
  print(summarytokenizer.decode(outputs[0], skip_special_tokens=True))

# def test_labels(text):
#   NER = spacy.load("en_core_web_sm")
#   # text = clean_and_preprocess_text(text)
#   # print(text)
#   text1= NER(text)
#   new_ents = []

#   for ent in text1.ents:
#       if not ent[0].is_lower:
#           new_ents.append(ent)

#   for entity in new_ents:
#    if entity.label_ == 'ORG':
#       print(entity.text)

# from nltk import word_tokenize, pos_tag, ne_chunk
# from nltk import Tree

# def get_continuous_chunks(text, label):
#     chunked = ne_chunk(pos_tag(word_tokenize(text)))
#     prev = None
#     continuous_chunk = []
#     current_chunk = []

#     for subtree in chunked:
#         if type(subtree) == Tree and subtree.label() == label:
#             current_chunk.append(" ".join([token for token, pos in subtree.leaves()]))
#         if current_chunk:
#             named_entity = " ".join(current_chunk)
#             if named_entity not in continuous_chunk:
#                 continuous_chunk.append(named_entity)
#                 current_chunk = []
#         else:
#             continue

#     return continuous_chunk

def initiate(job_description):
    pdf_directory = "./resumes"
    dic = iterate(pdf_directory, job_description)
    
    pdf_scores = {}
    for pdf_name, pdf_data in dic.items():
        scores = pdf_data.get("Scores", [])
        total_score = sum(scores)
        pdf_scores[pdf_name] = total_score

# Sort PDF names based on total scores (descending order)
    sorted_pdfs = sorted(pdf_scores, key=pdf_scores.get, reverse=True)
    dic["FINAL RANKING"] = sorted_pdfs


    json_string = json.dumps(dic)
    json_string = json_string.replace('\\"', '"')
    return json_string